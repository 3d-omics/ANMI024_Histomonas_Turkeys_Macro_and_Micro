---
title: "3D'omics | Histomonas Turkeys"
author:
- Amalia Bogri^[University of Copenhagen, amalia.bogri@sund.ku.dk]
- Jorge Langa^[University of Copenhagen, jorge.langa@sund.ku.dk]
- Antton Alberdi^[University of Copenhagen, antton.alberdi@sund.ku.dk]

date: "`r Sys.Date()`"
subtitle: Work in progress
site: bookdown::bookdown_site
documentclass: book
#bibliography:
#- book.bib
#- packages.bib
url: "https://github.com/3d-omics/ANMI024_Histomonas_Turkeys_Macro_and_Micro.git"
description: |
  Data analysis code for 3D'omics Histomoans trial in Turkeys
#link-citations: true
github-repo: "3d-omics/ANMI024_Histomonas_Turkeys_Macro_and_Micro"
---

```{r knitr_opts, echo=FALSE}
knitr::opts_chunk$set(
  class.source = "script-source",
  class.output = "script-output",
  comment = NA
)
```

# Introduction

This webbook contains all the code used for the analyses of for Deliverable 6.3 for the Histomonas trials on turkeys.

## Prepare the R environment

### Environment

To reproduce all the analyses locally, clone this repository in your computer using:

```
RStudio > New Project > Version Control > Git
```

And indicating the following git repository:

> https://github.com/3d-omics/ANMI024_Histomonas_Turkeys_Macro_and_Micro.git

Once the R project has been created, follow the instructions and code chunks shown in this webbook.

### Libraries

The following R packages are required for the data analysis.

```{r load_libraries, warning=FALSE, comments="", message=FALSE, eval=TRUE}
# Base
library(R.utils)
library(knitr)
library(devtools)
library(tinytable)
library(broom)
library(broom.mixed)
library(rairtable)
library(jsonlite)

# For tree handling
library(ape)
library(phyloseq)
library(phytools)

# For plotting
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(ggnewscale)
library(gridExtra)
library(ggtreeExtra)
library(ggtree)
library(ggh4x)
library(jpeg)
library(patchwork)

# For statistics
library(spaa)
library(vegan)
library(Rtsne)
library(geiger)
library(hilldiv2)
library(distillR)
library(lme4)
library(Hmsc)
library(psych)

# For compositional analyses
library(zCompositions)
library(compositions)
library(ALDEx2)
library(ANCOMBC)
library(microbiome)
library(ggcorrplot)
library(propr)

library(MASS) # for Negative Binomial Fit
library(ggVennDiagram)


library(janitor)
library(permuco)
library(magick)
library(ade4)
library(spdep)


library(Hmisc)
library(reshape2)
# Tidyverse libraries (load dplyr last to avoid conflicts)
library(tidyverse)
library(dplyr) # Explicitly load dplyr last
```

## Helper Functions

### Alpha Diversity Calculation

Function to estimate the genomes' alpha diversity measurements from genome counts.

```{r calculate_alpha_diversity, warning=FALSE, comments="", message=FALSE, eval=TRUE}
calculate_alpha_diversity <- function(input_data, dataset_name) {
  # Step 1: Transform the input data (remove rownames if needed)
  input_data_matrix <- input_data %>%
    column_to_rownames(var = "genome")  #%>%
   # mutate_at(vars(-genome), ~ . / sum(.))  # No reason to do this closure to 1. Results are the same if you do it or not.

  # Step 2: Calculate richness (q = 0)
  richness <- hilldiv(input_data_matrix, q = 0) %>%
    t() %>%
    as.data.frame() %>%
    rename(richness = 1) %>%
    rownames_to_column(var = "microsample")

  # Step 3: Calculate neutral diversity (q = 1)
  neutral <- hilldiv(input_data_matrix, q = 1) %>%
    t() %>%
    as.data.frame() %>%
    rename(neutral = 1) %>%
    rownames_to_column(var = "microsample")

  # Step 4: Calculate phylogenetic diversity (q = 1, with genome tree)
  phylogenetic <- hilldiv(input_data_matrix, q = 1, tree = genome_tree) %>%
    t() %>%
    as.data.frame() %>%
    rename(phylogenetic = 1) %>%
    rownames_to_column(var = "microsample")

  # Step 5: Merge all diversity metrics
  alpha_diversity <- richness %>%
    full_join(neutral, by = "microsample") %>%
    full_join(phylogenetic, by = "microsample") # %>%
    # left_join(sample_metadata, by = "microsample") %>%
    # left_join(final_combined_stats, by = "microsample") %>%

  # Return the alpha_diversity data frame
  return(alpha_diversity)
}
```


### Remove samples or taxa

```{r remove_samples_taxa, warning = FALSE, comments = "", message = FALSE, eval=TRUE}
# Input: A wide df with samples as rows and taxa as columns
# Result: Remove taxa that are in too few samples, or samples that have too few taxa

remove_samples_or_taxa <- function(df, min_samples_per_taxon, min_taxa_per_sample){
  # Store & print original df dimensions
  original_rows <- nrow(df)
  original_cols <- ncol(df)
  cat("Initial df: Rows (samples):", original_rows, ", Columns (taxa):", original_cols, "\n")

  # Remove taxa that are in less than x samples
  df <- df %>% select(where(~ sum(. != 0) > min_samples_per_taxon))
  
  # Remove samples that contain less than x taxa
  df <- df %>% filter(rowSums(. != 0) > min_taxa_per_sample)
  
  df < df %>% select(where(~ any(. != 0)) | where(is.character) | where(is.factor))  

  removed_rows <- original_rows - nrow(df)
  removed_cols <- original_cols - ncol(df)
  cat("Removed: Rows (samples):", removed_rows, ", Columns (taxa):", removed_cols, "\n")

  cat("Resulting df: Rows (samples):", nrow(df), ", Columns (taxa):", ncol(df), "\n")
  
  return(df)
}
```


### clr_function
```{r clr_function, warning = FALSE, comments = "", message = FALSE, eval=TRUE}
  clr_transform <- function(x) {
    log(x) - mean(log(x), na.rm = TRUE)
  }
```

### perform_pca
```{r pca_function, warning = FALSE, comments = "", message = FALSE, eval=TRUE}
perform_pca <- function(df, zero_method = "GBM", z_delete = TRUE) {
  # Store original dimensions
  original_rows <- nrow(df)
  original_cols <- ncol(df)

  # 1. Zero replacement
  if (any(df == 0)) { # I think cmultRepl already does that
    print("Zeros found")
    df <- cmultRepl(df,
      method = zero_method, output = "prop",
      z.warning = 0.8, z.delete = z_delete
    )
    df <- df * 100
  }

  # Print removed rows and columns
  removed_rows <- original_rows - nrow(df)
  removed_cols <- original_cols - ncol(df)
  cat("Rows (samples) removed after zero replacement:", removed_rows, "\n")
  cat("Columns (taxa) removed after zero replacement:", removed_cols, "\n")

  # Geometric mean function
  geometric_mean <- function(x) {
    # Use log to avoid underflow
    exp(mean(log(x), na.rm = TRUE))
  }

  # 2. Calculate geometric mean of the parts (taxa) of the data set.
  taxa_geometric_means <- apply(df, 2, geometric_mean)

  # 3. Center data
  df_centered <- sweep(df, 2, taxa_geometric_means, FUN = "/")

  df_centered <- as.matrix(df_centered)

  # Compute the Variation Matrix
  variation_matrix <- outer(
    1:ncol(df_centered), 1:ncol(df_centered),
    Vectorize(function(i, j) var(log(df_centered[, i] / df_centered[, j]), na.rm = TRUE))
  )

  # Calculate Total Variance
  D <- ncol(df_centered) # Number of taxa (columns)
  totvar <- (1 / (2 * D)) * sum(variation_matrix, na.rm = TRUE)

  # 4. Scale data
  power_exponent <- 1 / sqrt(totvar)
  df_scaled <- df_centered^power_exponent

  # CLR transform data
  df_clr <- as.data.frame(t(apply(df_scaled, 1, clr_transform)))

  df_clr_dist <- as.data.frame(t(apply(df, 1, clr_transform)))

  # Perform PCA on zero replaced, centered, scaled, and CLR transformed df
  pca_result <- prcomp(df_clr, center = FALSE, scale. = FALSE)

  return(list(
    df_clr = df_clr,
    df_clr_dist = df_clr_dist,
    pca_result = pca_result
  ))
}
```

### plot_pca
```{r PCA_plot_function, warning=FALSE, comments="", message=FALSE, eval=TRUE}
# Use the 'pca_result' df produced from 'perform_pca' function to make the PCA plot

plot_pca <- function(df, 
                     samples_color_metadata, samples_shape_metadata, 
                     samples_color_value, loadings_color_metadata, 
                     loadings_color_value, loadings_taxon_level,
                     sample_metadata, genome_metadata, order_colors, 
                     custom_ggplot_theme, scaling_factor_value = 1.5, 
                     loadings_number = 10, show_labels = FALSE, add_arrows = FALSE, add_centroids = FALSE) {
  
  # Extract scores from PCA results
  scores <- rownames_to_column(as.data.frame(df$x), var = "microsample")
  scores <- left_join(scores, sample_metadata, by = join_by(microsample == microsample))
  
  # Calculate limits for x and y axes
  x_limit <- max(abs(scores$PC1))
  y_limit <- max(abs(scores$PC2))
  
  # Calculate variance explained by each PC (principal component) & create labels for plot
  variance_explained <- (df$sdev^2) / sum(df$sdev^2) * 100
  pc1_label <- paste0("PC1: ", round(variance_explained[1], 2), "% variance explained")
  pc2_label <- paste0("PC2: ", round(variance_explained[2], 2), "% variance explained")
  
  # Set a scaling factor for loadings (arrows)
  scaling_factor <- scaling_factor_value
  
  # Extract and scale the loadings
  loadings <- df$rotation[, 1:2] %>%
    as.data.frame() %>%
    mutate(genome = rownames(.)) %>%  
    mutate(PC1 = PC1 * scaling_factor,
           PC2 = PC2 * scaling_factor) %>%
    left_join(genome_metadata, by = join_by(genome == genome)) %>%  
    mutate(abs_loading = sqrt(PC1^2 + PC2^2)) %>%  
    arrange(desc(abs_loading)) %>%
    slice_max(order_by = abs_loading, n = loadings_number) %>%
    mutate(order_color = order_colors[order])
  
  # Create ggplot
  p <- ggplot() +
    # Plot the samples (points)
    geom_point(data = scores, 
               aes(x = PC1, y = PC2, 
                   fill = .data[[samples_color_metadata]],
                   shape = .data[[samples_shape_metadata]]), #.data[[]] tells ggplot2 to look up the column dynamically.
               size = 2.5, alpha = 0.8,
               color = "black", stroke = 0.3) +
    
    scale_fill_manual(values = samples_color_value) + 
    scale_shape_manual(values = c(21, 24, 23, 22, 25)) +
    new_scale_color() + #  new_scale_fill() ?
    # Plot the loadings (taxa, i.e. arrows)
    geom_segment(data = loadings, 
                 aes(x = 0, y = 0, xend = PC1, yend = PC2, color = .data[[loadings_color_metadata]]),
                 arrow = arrow(length = unit(0.2, "cm")), 
                 size = 0.7, alpha = 0.9) +
    scale_color_manual(name = "Classification", values = loadings_color_value) +
    geom_text_repel(data = loadings, 
                    aes(x = PC1, y = PC2, label = .data[[loadings_taxon_level]]),
                    color = "black", size = 3, vjust = -0.5, alpha=0.7, max.overlaps = 20) +
    labs(title = "PCA Ordination Plot",
         x = pc1_label,
         y = pc2_label) +
    scale_x_continuous(limits = c(-x_limit, x_limit)) +
    scale_y_continuous(limits = c(-y_limit, y_limit)) +
    geom_hline(yintercept = 0, color = "darkgrey") +
    geom_vline(xintercept = 0, color = "darkgrey") +
    theme_minimal() +
    custom_ggplot_theme +
    # coord_fixed(ratio = 1) +
    guides(fill = guide_legend(override.aes = list(shape = 21, color = "black"))) # to print the legend for colour correctly!
  
  
      # ADD text to points
     if (show_labels) {
        p <- p + 
          geom_text(
            data = scores, 
            aes(x = PC1, y = PC2, label = animal),
            position = position_nudge(y = -0.02),
            size = 3.5, alpha = 0.7
          )
     }
  
  if (add_arrows) {
  # Prepare arrow data
  arrows_df <- scores %>%
    arrange(treatment, age) %>%
    group_by(treatment) %>%
    mutate(
      PC1_next = lead(PC1),
      PC2_next = lead(PC2)
    ) %>%
    filter(!is.na(PC1_next))
  
  # Add arrows to the plot
  p <- p + 
    geom_segment(
      data = arrows_df,
      aes(
        x = PC1, y = PC2,
        xend = PC1_next, yend = PC2_next,
        color = treatment
      ),
      arrow = arrow(length = unit(0.15, "cm")),
      linewidth = 0.6,
      alpha = 0.8,
      inherit.aes = FALSE
    )
  }
  
if (add_centroids) {

  # Compute centroids per treatment Ã— age
  centroids <- scores %>%
    group_by(.data[[samples_color_metadata]], .data[[samples_shape_metadata]]) %>%
    summarise(
      PC1 = mean(PC1),
      PC2 = mean(PC2),
      .groups = "drop"
    ) %>%
    rename(
      color_group = 1,
      shape_group = 2
    ) %>%
    arrange(color_group, shape_group)

  centroid_arrows <- centroids %>%
    group_by(color_group) %>%
    mutate(
      PC1_next = lead(PC1),
      PC2_next = lead(PC2)
    ) %>%
    filter(!is.na(PC1_next))

  # Add new color scale so centroids use treatment palette again
  p <- p + 
    new_scale_color() +
    new_scale_fill() +

    geom_segment(
      data = centroid_arrows,
      aes(
        x = PC1, y = PC2,
        xend = PC1_next, yend = PC2_next,
        color = color_group
      ),
      arrow = arrow(length = unit(0.25, "cm"), type = "closed"),
      linewidth = 1.2,
      alpha = 0.7,
      inherit.aes = FALSE
    ) +
    geom_point(
      data = centroids,
      aes(
        x = PC1, y = PC2,
        shape = factor(shape_group),
        color = color_group,
        fill  = color_group
      ),
      size = 4.5,
      stroke = 1,
      alpha = 0.5,
      fill = "white",
      inherit.aes = FALSE
    ) +
    geom_text(
      data = centroids,
      aes(
        x = PC1, y = PC2,
        label = shape_group,
        color = color_group
      ),
      size = 3.3,
      vjust = -1.2,
      fontface = "bold",
      inherit.aes = FALSE
    ) +
    scale_color_manual(values = samples_color_value) +
    scale_fill_manual(values = samples_color_value)
}
  
  return(p)
}
```




## Notes:
### Installation of modules
Most of the modules can be downloaded from CRAN with 'install.packages('XXX', dependencies = TRUE)'.
Four modules are from Bioconductor. 
Install with: 'install.packages('BiocManager', dependencies = TRUE)
BiocManager::install(c("phyloseq", "ggtreeExtra","ggtree","ANCOMBC"))'
Two modules are from Antton's github. 
Install with: 'install.packages('remotes', dependencies = TRUE)
remotes::install_github('anttonalberdi/hilldiv2')
remotes::install_github('anttonalberdi/distillR')'
# if (!require("BiocManager", quietly = TRUE))
#      install.packages("BiocManager")
# BiocManager::install("ALDEx2")
# remotes::install_github('tpq/propr')


### Airtable access:
Currently the metadata is loaded from the 3D'omics airtable. In order to access it you need your own API key (from Antton). set_airtable_api_key('XXXXX', install = TRUE)
